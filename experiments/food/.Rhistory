all.foods = as.character(df[i, cs.names])
choice.ind = 1
val = as.numeric(as.character(df[i, val.names[1]]))
val.spec = as.numeric(as.character(df[i, val.spec.names[1]]))
df2 = rbind(df2, data.frame(subject = subj, food = choice, chosen = 1, val = val, val.spec = val.spec, choice = choice))
for (j in 1:length(cs.names)) {
food = as.character(df[i, cs.names[j]])
if (nchar(food) > 0) {
val = as.numeric(as.character(df[i, val.names[j+1]]))
val.spec = as.numeric(as.character(df[i, val.spec.names[j+1]]))
chosen = 0
df2 = rbind(df2, data.frame(subject = subj, food = food, chosen = chosen, val = val, val.spec = val.spec, choice = choice))
}
}
}
# exclude people
df.subj = df2 %>% group_by(subject) %>%
summarize(cs.size = n(), nas = any(is.na(val) | is.na(val.spec)),
var.choice = sd(c(val, val.spec)),
val = mean(val), val.spec = mean(val.spec))
df2.filt = df2 %>% filter(!(subject %in% df.subj$subject[df.subj$nas == T | df.subj$var.choice == 0]))
df2.subj.filt = df.subj %>% filter(!nas & var.choice != 0)
# check out value distributions of generated options
pct.gen.val.high = mean(df2.filt$val > 4)
pct.gen.val.high
ci(pct.gen.val.high, se.p(df2.filt$val > 4))
pct.gen.val.low = mean(df2.filt$val < 4)
pct.gen.val.low
ci(pct.gen.val.low, se.p(df2.filt$val < 4))
pct.gen.valspec.high = mean(df2.filt$val.spec > 4)
pct.gen.valspec.high
ci(pct.gen.valspec.high, se.p(df2.filt$val.spec > 4))
pct.gen.valspec.low = mean(df2.filt$val.spec < 4)
pct.gen.valspec.low
ci(pct.gen.valspec.low, se.p(df2.filt$val.spec < 4))
# check out value distributions of chosen options
pct.chosen.val.high = mean(df2.filt$val[df2.filt$chosen == 1] > 4)
pct.chosen.val.high
ci(pct.chosen.val.high, se.p(df2.filt$val[df2.filt$chosen == 1] > 4))
pct.chosen.val.low = mean(df2.filt$val[df2.filt$chosen == 1] < 4)
pct.chosen.val.low
ci(pct.chosen.val.low, se.p(df2.filt$val[df2.filt$chosen == 1] < 4))
pct.chosen.valspec.high = mean(df2.filt$val.spec[df2.filt$chosen == 1] > 4)
pct.chosen.valspec.high
ci(pct.chosen.valspec.high, se.p(df2.filt$val.spec[df2.filt$chosen == 1] > 4))
pct.chosen.valspec.low = mean(df2.filt$val.spec[df2.filt$chosen == 1] < 4)
pct.chosen.valspec.low
ci(pct.chosen.valspec.low, se.p(df2.filt$val.spec[df2.filt$chosen == 1] < 4))
# test how correlated general & specific value are
cor.test(df2.filt$val, df2.filt$val.spec)
# graph raw data (for supplement)
p = ggplot(data = df2.filt %>% mutate(chosen = ifelse(chosen == T, T, NA)),
aes(x = val, y = val.spec, color = chosen, group = chosen)) +
geom_jitter(size = 2) +
geom_vline(xintercept = 4) +
geom_hline(yintercept = 4) +
labs(x = '', y = '') +
guides(color = F)
ggMarginal(p, type='histogram', xparams = list(bins=8), yparams = list(bins = 8))
# test whether general value > midpoint of scale (4), controlling for specific value
m1 = lmer(val ~ val.spec + (val.spec | subject), data = df2.filt %>% mutate(val = val - 4, val.spec = val.spec - 4))
summary(m1)
# test whether specific value > midpoint of scale (4), controlling for general value
m2 = lmer(val.spec ~ val + (val | subject), data = df2.filt %>% mutate(val = val - 4, val.spec = val.spec - 4))
summary(m2)
# add ranks
subjs = unique(df2.filt$subject)
for (i in 1:length(subjs)) {
subj = subjs[i]
df2.rows = df2.filt$subject == subj
vals = -df2.filt$val[df2.rows]
vals.spec = -df2.filt$val.spec[df2.rows]
df2.filt$val.rank[df2.rows] = rank(vals, ties.method = 'min')
df2.filt$val.spec.rank[df2.rows] = rank(vals.spec, ties.method = 'min')
}
df2.filt = df2.filt %>%
mutate(val.rank = ifelse(val.rank > 5, 5, val.rank), val.spec.rank = ifelse(val.spec.rank > 5, 5, val.spec.rank),
val.rank = max(val.rank) - val.rank + 1, val.spec.rank = max(val.spec.rank) - val.spec.rank + 1)
# assign every other midpoint to high or low (for dichotomization)
c1 = 0
c2 = 0
for (i in 1:nrow(df2.filt)) {
if (df2.filt$val[i] == 4) {
df2.filt$val.four[i] = c1
c1 = ifelse(c1 == 0, 1, 0)
} else {
df2.filt$val.four[i] = -1
}
if (df2.filt$val.spec[i] == 4) {
df2.filt$val.spec.four[i] = c2
c2 = ifelse(c2 == 0, 1, 0)
} else {
df2.filt$val.spec.four[i] = -1
}
}
df2.filt = df2.filt %>%
mutate(val.adj = ifelse(val == 4, ifelse(val.four == 0, 3.9, 4.1), val),
val.spec.adj = ifelse(val == 4, ifelse(val.spec.four == 0, 3.9, 4.1), val.spec),
val.fac = factor(val.adj > 4, c(F,T), c('Low', 'High')),
val.spec.fac = factor(val.spec.adj > 4, c(F,T), c('Low', 'High')))
# for graph, do marginal probability of each
graph.cs = df2.filt %>% select(subject, val.fac, val.spec.fac) %>%
rename(val = val.fac, val.spec = val.spec.fac) %>%
gather("type", "rank", -subject) %>%
group_by(type, subject) %>%
summarize(low = mean(rank == 'Low'), high = mean(rank == 'High')) %>%
gather("rank", "prob", -c(subject,type)) %>%
group_by(type, rank) %>%
summarize(prob.m = mean(prob), prob.se = se(prob)) %>%
group_by() %>%
mutate(type = factor(type, c('val', 'val.spec')), rank = factor(rank, c('low', 'high')))
ggplot(graph.cs, aes(x = rank, y = prob.m, group = type, color = type)) +
geom_point(size = 4) +
geom_line(size = 1.5) +
geom_errorbar(aes(ymin = prob.m - prob.se, ymax = prob.m + prob.se), width = .2) +
xlab('') + ylab('') +
scale_x_discrete(labels = NULL) +
scale_y_continuous(limits = c(.15,.85), breaks = c(.2, .8)) +
theme(legend.position = 'none') +
scale_color_manual(values = c('#d11a02', '#105db0'))
# for stats, do it more precisely: controlling for each other
df.model.cs = df2.filt %>% group_by(subject) %>%
summarize(ll = mean(val.fac == 'Low' & val.spec.fac == 'Low'),
lh = mean(val.fac == 'Low' & val.spec.fac == 'High'),
hl = mean(val.fac == 'High' & val.spec.fac == 'Low'),
hh = mean(val.fac == 'High' & val.spec.fac == 'High')) %>%
gather("type", "prob", -subject) %>%
mutate(val.high = type %in% c('hl', 'hh'), val.spec.high = type %in% c('lh', 'hh'))
m.cs = lmer(prob ~ val.high + val.spec.high + (1 | subject), data = df.model.cs)
summary(m.cs)
std_beta(m.cs)
# graph it
df.chosen.mf = df2.filt %>% group_by(val.rank, subject) %>%
summarize(chosen = mean(chosen)) %>%
group_by(val.rank) %>%
summarize(chosen.m = mean(chosen), chosen.se = se(chosen))
df.chosen.mb = df2.filt %>% group_by(val.spec.rank, subject) %>%
summarize(chosen = mean(chosen)) %>%
group_by(val.spec.rank) %>%
summarize(chosen.m = mean(chosen), chosen.se = se(chosen))
df.chosen = rbind(df.chosen.mf, df.chosen.mb %>% mutate(val.rank = val.spec.rank) %>% select(-val.spec.rank)) %>%
mutate(type = rep(c(0,1), each = nrow(df.chosen.mf)), type.fac = factor(type, c(0,1), c('MF', 'MB')))
ggplot(df.chosen, aes(x = val.rank, y = chosen.m, color = type.fac, group = type.fac)) +
geom_point(size = 3) +
geom_line(size = 1.5) +
geom_errorbar(aes(ymin = chosen.m - chosen.se, ymax = chosen.m + chosen.se), width = .2) +
xlab('') +
ylab('') +
scale_color_manual(values = c('#d11a02', '#105db0')) +
scale_y_continuous(breaks = c(0, .6), limits = c(0, .67)) +
scale_x_continuous(labels = NULL, breaks = c(1,5)) +
theme(legend.position = 'none')
# run multinomial logit
df2.logit = df2.filt %>% mutate(subject = as.numeric(subject))
for (i in 1:nrow(df2.logit)) {
s = df2.logit$subject[i]
if (i == 1 || s != last.s) {
fc = 1
}
df2.logit$food.id[i] = fc
fc = fc + 1
last.s = s
}
df2.logit2 = mlogit.data(df2.logit, choice = "chosen", shape = "long", id.var = "subject", alt.var = "food.id", chid.var = 'subject')
m.selection = mlogit(chosen ~ val.rank + val.spec.rank | -1, df2.logit2, panel = T,
rpar = c(val.rank = "n", val.spec.rank = "n"), halton = NA, R = 1000, tol = .001)
m.selection.summary = summary(m.selection)
m.selection.summary
# For the mlogit model, there was no previous function to get the standardized coefficients, so I just did it myself.
# I'm not dividing by the sd of the dependent variable because it doesn't make sense for a multinomial logistic regression.
m.selection.summary$CoefTable[1,1] * sd(df2.logit2$val.rank)
m.selection.summary$CoefTable[1,2] * sd(df2.logit2$val.rank)
m.selection.summary$CoefTable[2,1] * sd(df2.logit2$val.spec.rank)
m.selection.summary$CoefTable[2,2] * sd(df2.logit2$val.spec.rank)
# get Bayes factor for null (used in Study 2)
m.selection.null = mlogit(chosen ~ val.spec.rank | -1, df2.logit2, panel = T,
rpar = c(val.spec.rank = "n"), halton = NA, R = 1000, tol = .001)
summary(m.selection.null)
ll1 = logLik(m.selection)
BIC1 = attr(ll1, 'df') * log(length(m.selection$fitted.values)) - 2 * as.numeric(ll1)
ll0 = logLik(m.selection.null)
BIC0 = attr(ll0, 'df') * log(length(m.selection.null$fitted.values)) - 2 * as.numeric(ll0)
BFnull.s1choice = exp((BIC1 - BIC0) / 2)
BFnull.s1choice
1 / BFnull.s1choice
save.image(expt.output)
rm(list=ls())
load("/Users/adam/Me/Psychology/Projects/choicesets/git2/experiments/food/study3_output.rdata")
rm(list=ls())
# load packages with groundhog (http://groundhogr.com/)
# if you get this error:
# "groundhog says: 11 of the 21 packages needed by 'dplyr_0.8.4' are currently loaded, but not with the version that is needed."
# then run "rm(list=ls())", restart your R session, and try again.
#  if you still get the error, then do all the following steps in order:
# switch to R version 4, restart your R session, run "library(groundhog); groundhog.library('dplyr', '2020-06-01')", restart your R session, run "library(groundhog); groundhog.library('dplyr', '2020-03-01')", switch back to R version 3, restart your R session, and try running this script again.
# (I think this is a bug in groundhogr, and I have no idea why this fixes it, but that's what worked for me.)
library(groundhog)
pkgs = c('dplyr', 'tidyr', 'ggplot2', 'ggExtra', 'lme4', 'lmerTest', 'mlogit', 'stringdist', 'rje')
groundhog.library(pkgs, '2020-03-01')
# load packages with groundhog (http://groundhogr.com/)
# if you get this error:
# "groundhog says: 11 of the 21 packages needed by 'dplyr_0.8.4' are currently loaded, but not with the version that is needed."
# then run "rm(list=ls())", restart your R session, and try again.
#  if you still get the error, then do all the following steps in order:
# switch to R version 4, restart your R session, run "library(groundhog); groundhog.library('dplyr', '2020-06-01')", restart your R session, run "library(groundhog); groundhog.library('dplyr', '2020-03-01')", switch back to R version 3, restart your R session, and try running this script again.
# (I think this is a bug in groundhogr, and I have no idea why this fixes it, but that's what worked for me.)
library(groundhog)
pkgs = c('dplyr', 'tidyr', 'ggplot2', 'ggExtra', 'lme4', 'lmerTest', 'mlogit', 'stringdist', 'rje')
groundhog.library(pkgs, '2020-03-01')
# Only works in RStudio -- otherwise you have to set the path manually
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# Update ggplot theme
theme_update(strip.background = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
plot.background = element_blank(),
axis.text=element_text(size=30, colour = "black"),
axis.title=element_text(size=18, face = "bold"),
axis.title.x = element_text(vjust = 0),
legend.title = element_text(size = 24, face = "bold"),
legend.text = element_text(size = 20),
plot.title = element_text(size = 26, face = "bold", vjust = 1),
panel.margin = unit(1.0, "lines"),
plot.margin = unit(c(0.5,  0.5, 0.5, 0.5), "lines"),
axis.line = element_line(colour = "black", size = 2),
axis.ticks = element_line(color = 'black', size = 3),
axis.ticks.length = unit(.25, 'cm')
)
# Make some handy functions
se = function(x) {return(sd(x, na.rm = T) / sqrt(sum(!is.na(x))))}
dodge <- position_dodge(width=0.9)
betterLine = function(data, formula, color = '#105db0') {
lg = summary(lm(formula, data))$coefficients
return(c(geom_abline(intercept = lg[1,1], slope = lg[2,1], color = color, size = 1.25),
geom_abline(intercept = lg[1,1] - lg[1,2], slope = lg[2,1], color = color, linetype = 'dashed', size = 1, alpha = .8),
geom_abline(intercept = lg[1,1] + lg[1,2], slope = lg[2,1], color = color, linetype = 'dashed', size = 1, alpha = .8)))
}
# This function is to compute standardized beta coefficients from a mixed effects model.
# It is copied verbatim from the package 'sjstats', version 0.17.2.
# The reason it's copied here (instead of loading the package) is that, when I did the analysis,
# I was using this latest version of sjstats, but with an old version of R (3.6.3).
# Groundhog won't let me use the latest version with an old version of R, and the older version was giving different answers,
# so I decided the simplest solution was just to copy the relevant function into my script.
# Citation:
# LÃ¼decke D (2019). _sjstats: Statistical Functions for Regression Models (Version 0.17.5)_. doi: 10.5281/zenodo.1284472 (URL: https://doi.org/10.5281/zenodo.1284472), <URL: https://CRAN.R-project.org/package=sjstats>.
std_beta <- function(fit, ci.lvl = .95, ...) {
# compute ci, two-ways
ci <- 1 - ((1 - ci.lvl) / 2)
# code from Ben Bolker, see
# http://stackoverflow.com/a/26206119/2094622
sdy <- stats::sd(lme4::getME(fit, "y"))
sdx <- apply(lme4::getME(fit, "X"), 2, sd)
sc <- lme4::fixef(fit) * sdx / sdy
se.fixef <- stats::coef(summary(fit))[, "Std. Error"]
se <- se.fixef * sdx / sdy
data_frame(
term = names(lme4::fixef(fit)),
std.estimate = sc,
std.error = se,
conf.low = sc - stats::qnorm(ci) * se,
conf.high = sc + stats::qnorm(ci) * se
) %>%
dplyr::slice(-1)
}
expt = 3
# import data -------------------------------------------------------------
expt.data = paste0('study', expt, '_data.csv')
expt.output = paste0('study', expt, '_output.rdata')
df = read.csv(expt.data, stringsAsFactors = F) %>% filter(DistributionChannel != 'preview', Status == 0) %>%
dplyr::select(ResponseId, choice_30, choicetime_30_Page.Submit,
cs_1, cs_3, cs_4, cs_5, cs_6, cs_7, cs_8, cs_13,
cs_time_Page.Submit,
val_q_1, val_q_2, val_q_3, val_q_4, val_q_5, val_q_6, val_q_7, val_q_8, val_q_9,
val_time_Page.Submit,
val_other_q_1, val_other_q_2, val_other_q_3, val_other_q_4, val_other_q_5, val_other_q_6, val_other_q_7, val_other_q_8, val_other_q_9,
Q159_Page.Submit,
freq_q_1, freq_q_2, freq_q_3, freq_q_4, freq_q_5, freq_q_6, freq_q_7, freq_q_8, freq_q_9,
freq_q_Page.Submit)
df2 = data.frame(subject = character(), food = character(), chosen = logical(), val = numeric(), val.spec = numeric(), choice = character())
cs.names = c('cs_1', 'cs_3', 'cs_4', 'cs_5', 'cs_6', 'cs_7', 'cs_8', 'cs_13')
val.names = c('val_q_1', 'val_q_2', 'val_q_3', 'val_q_4', 'val_q_5', 'val_q_6', 'val_q_7', 'val_q_8', 'val_q_9')
val.spec.names = c('val_other_q_1', 'val_other_q_2', 'val_other_q_3', 'val_other_q_4', 'val_other_q_5', 'val_other_q_6', 'val_other_q_7', 'val_other_q_8', 'val_other_q_9')
freq.names = c('freq_q_1', 'freq_q_2', 'freq_q_3', 'freq_q_4', 'freq_q_5', 'freq_q_6', 'freq_q_7', 'freq_q_8', 'freq_q_9')
for (i in 1:nrow(df)) {
subj = df$ResponseId[i]
choice = as.character(df$choice_30[i])
all.foods = as.character(df[i, cs.names])
choice.ind = 1
val = as.numeric(as.character(df[i, val.names[1]]))
val.spec = as.numeric(as.character(df[i, val.spec.names[1]]))
freq = as.numeric(as.character(df[i, freq.names[1]]))
df2 = rbind(df2, data.frame(subject = subj, food = choice, chosen = 1, val = val, val.spec = val.spec, freq = freq, choice = choice))
for (j in 1:length(cs.names)) {
food = as.character(df[i, cs.names[j]])
if (nchar(food) > 0) {
val = as.numeric(as.character(df[i, val.names[j+1]]))
val.spec = as.numeric(as.character(df[i, val.spec.names[j+1]]))
freq = as.numeric(as.character(df[i, freq.names[j+1]]))
chosen = 0
df2 = rbind(df2, data.frame(subject = subj, food = food, chosen = chosen, val = val, val.spec = val.spec, freq = freq, choice = choice))
}
}
}
df.subj = df2 %>% group_by(subject) %>%
summarize(cs.size = n(), nas = any(is.na(val) | is.na(val.spec) | is.na(freq)),
var.choice = sd(c(val, val.spec, freq)),
val = mean(val), val.spec = mean(val.spec), freq = mean(freq))
df2.filt = df2 %>% filter(!(subject %in% df.subj$subject[df.subj$nas == T | df.subj$var.choice == 0]))
df2.subj.filt = df.subj %>% filter(!nas & var.choice != 0)
# test how correlated general & specific value are
cor.test(df2.filt$val, df2.filt$val.spec)
cor.test(df2.filt$val, df2.filt$freq)
cor.test(df2.filt$val.spec, df2.filt$freq)
# plot raw data
p = ggplot(data = df2.filt %>% mutate(chosen = ifelse(chosen == T, T, NA)), aes(x = val, y = val.spec, color = chosen, group = chosen)) +
geom_jitter(size = 2) +
geom_vline(xintercept = 4) +
geom_hline(yintercept = 4) +
labs(x = '', y = '') +
guides(color = F)
ggMarginal(p, type='histogram', xparams = list(bins=8), yparams = list(bins = 8))
p = ggplot(data = df2.filt %>% mutate(chosen = ifelse(chosen == T, T, NA)), aes(x = freq, y = val, color = chosen, group = chosen)) +
geom_jitter(size = 2) +
geom_vline(xintercept = 4) +
geom_hline(yintercept = 4) +
labs(x = '', y = '') +
guides(color = F)
ggMarginal(p, type='histogram', xparams = list(bins=8), yparams = list(bins = 8))
p = ggplot(data = df2.filt %>% mutate(chosen = ifelse(chosen == T, T, NA)), aes(x = freq, y = val.spec, color = chosen, group = chosen)) +
geom_jitter(size = 2) +
geom_vline(xintercept = 4) +
geom_hline(yintercept = 4) +
labs(x = '', y = '') +
guides(color = F)
ggMarginal(p, type='histogram', xparams = list(bins=8), yparams = list(bins = 8))
# test whether general value > midpoint of scale (4), controlling for specific value
m1 = lmer(val ~ val.spec + (val.spec | subject), data = df2.filt %>% mutate(val = val - 4, val.spec = val.spec - 4))
summary(m1)
# test whether specific value > midpoint of scale (4), controlling for general value
m2 = lmer(val.spec ~ val + (val | subject), data = df2.filt %>% mutate(val = val - 4, val.spec = val.spec - 4))
summary(m2)
p.freq = ggplot(data = df2, aes(x = val, y = freq)) +
geom_jitter(size = 2) +
geom_vline(xintercept = 4) +
geom_hline(yintercept = 4) +
labs(x = '', y = '')
ggMarginal(p.freq, type='histogram', xparams = list(bins=8), yparams = list(bins = 8))
p.freq2 = ggplot(data = df2, aes(x = val.spec, y = freq)) +
geom_jitter(size = 2) +
geom_vline(xintercept = 4) +
geom_hline(yintercept = 4) +
labs(x = '', y = '')
ggMarginal(p.freq2, type='histogram', xparams = list(bins=8), yparams = list(bins = 8))
# test whether general value > midpoint (4), controlling for freq
m1 = lmer(val ~ freq + (freq | subject), data = df2 %>% mutate(val = val - 4, freq = freq - 4))
summary(m1)
# test whether freq > midpoint (4), controlling for value
m2 = lmer(freq ~ val + (val | subject), data = df2 %>% mutate(val = val - 4, freq = freq - 4))
summary(m2)
# add ranks
subjs = unique(df2.filt$subject)
for (i in 1:length(subjs)) {
subj = subjs[i]
df2.rows = df2.filt$subject == subj
vals = -df2.filt$val[df2.rows]
vals.spec = -df2.filt$val.spec[df2.rows]
freqs = -df2.filt$freq[df2.rows]
df2.filt$val.rank[df2.rows] = rank(vals, ties.method = 'min')
df2.filt$val.spec.rank[df2.rows] = rank(vals.spec, ties.method = 'min')
df2.filt$freq.rank[df2.rows] = rank(freqs, ties.method = 'min')
}
df2.filt = df2.filt %>%
mutate(val.rank = ifelse(val.rank > 5, 5, val.rank), val.spec.rank = ifelse(val.spec.rank > 5, 5, val.spec.rank),
freq.rank = ifelse(freq.rank > 5, 5, freq.rank),
val.rank = max(val.rank) - val.rank + 1, val.spec.rank = max(val.spec.rank) - val.spec.rank + 1,
freq.rank = max(freq.rank) - freq.rank + 1)
# assign every other midpoint to high or low (for dichotomization)
c1 = 0
c2 = 0
c3 = 0
for (i in 1:nrow(df2.filt)) {
if (df2.filt$val[i] == 4) {
df2.filt$val.four[i] = c1
c1 = ifelse(c1 == 0, 1, 0)
} else {
df2.filt$val.four[i] = -1
}
if (df2.filt$val.spec[i] == 4) {
df2.filt$val.spec.four[i] = c2
c2 = ifelse(c2 == 0, 1, 0)
} else {
df2.filt$val.spec.four[i] = -1
}
if (df2.filt$freq[i] == 4) {
df2.filt$freq.four[i] = c3
c3 = ifelse(c3 == 0, 1, 0)
} else {
df2.filt$freq.four[i] = -1
}
}
df2.filt = df2.filt %>%
mutate(val.adj = ifelse(val == 4, ifelse(val.four == 0, 3.9, 4.1), val),
val.spec.adj = ifelse(val == 4, ifelse(val.spec.four == 0, 3.9, 4.1), val.spec),
val.fac = factor(val.adj > 4, c(F,T), c('Low', 'High')),
val.spec.fac = factor(val.spec.adj > 4, c(F,T), c('Low', 'High')),
freq.adj = ifelse(val == 4, ifelse(freq.four == 0, 3.9, 4.1), freq),
freq.fac = factor(freq.adj > 4, c(F,T), c('Low', 'High')))
# for graph, do marginal probability
graph.cs = df2.filt %>% select(subject, val.fac, val.spec.fac, freq.fac) %>%
rename(val = val.fac, val.spec = val.spec.fac, freq = freq.fac) %>%
gather("type", "rank", -subject) %>%
group_by(type, subject) %>%
summarize(low = mean(rank == 'Low'), high = mean(rank == 'High')) %>%
gather("rank", "prob", -c(subject,type)) %>%
group_by(type, rank) %>%
summarize(prob.m = mean(prob), prob.se = se(prob)) %>%
group_by() %>%
mutate(type = factor(type, c('val', 'val.spec', 'freq')), rank = factor(rank, c('low', 'high')))
ggplot(graph.cs, aes(x = rank, y = prob.m, group = type, color = type)) +
geom_point(size = 4) +
geom_line(size = 1.5) +
geom_errorbar(aes(ymin = prob.m - prob.se, ymax = prob.m + prob.se), width = .2, linetype = 1) +
xlab('') + ylab('') +
scale_x_discrete(labels = NULL) +
scale_y_continuous(limits = c(.15,.85), breaks = c(.2, .8)) +
theme(legend.position = 'none') +
scale_color_manual(values = c('#d11a02', '#105db0', '#654321'))
# for stats, we're going to do it controlling for each other
df.model.cs = df2.filt %>% group_by(subject) %>%
summarize(ll = mean(val.fac == 'Low' & val.spec.fac == 'Low' & freq.fac == 'Low'),
lh = mean(val.fac == 'Low' & val.spec.fac == 'High' & freq.fac == 'Low'),
hl = mean(val.fac == 'High' & val.spec.fac == 'Low' & freq.fac == 'Low'),
hh = mean(val.fac == 'High' & val.spec.fac == 'High' & freq.fac == 'Low'),
llh = mean(val.fac == 'Low' & val.spec.fac == 'Low' & freq.fac == 'High'),
lhh = mean(val.fac == 'Low' & val.spec.fac == 'High' & freq.fac == 'High'),
hlh = mean(val.fac == 'High' & val.spec.fac == 'Low' & freq.fac == 'High'),
hhh = mean(val.fac == 'High' & val.spec.fac == 'High' & freq.fac == 'High')) %>%
gather("type", "prob", -subject) %>%
mutate(val.high = type %in% c('hl', 'hh', 'hlh', 'hhh'), val.spec.high = type %in% c('lh', 'hh', 'lhh', 'hhh'),
freq.high = type %in% c('llh', 'lhh', 'hlh', 'hhh'))
m.cs = lmer(prob ~ val.high + val.spec.high + freq.high + (1 | subject), data = df.model.cs)
summary(m.cs)
std_beta(m.cs)
df.chosen.mf = df2.filt %>% group_by(val.rank, subject) %>%
summarize(chosen = mean(chosen)) %>%
group_by(val.rank) %>%
summarize(chosen.m = mean(chosen), chosen.se = se(chosen))
df.chosen.mb = df2.filt %>% group_by(val.spec.rank, subject) %>%
summarize(chosen = mean(chosen)) %>%
group_by(val.spec.rank) %>%
summarize(chosen.m = mean(chosen), chosen.se = se(chosen))
df.chosen.freq = df2.filt %>% group_by(freq.rank, subject) %>%
summarize(chosen = mean(chosen)) %>%
group_by(freq.rank) %>%
summarize(chosen.m = mean(chosen), chosen.se = se(chosen))
df.chosen = rbind(df.chosen.mf, df.chosen.mb %>% mutate(val.rank = val.spec.rank) %>% select(-val.spec.rank),
df.chosen.freq %>% mutate(val.rank = freq.rank) %>% select(-freq.rank)) %>%
mutate(type = rep(c(0,1,2), each = nrow(df.chosen.mf)), type.fac = factor(type, c(0,1,2), c('MF', 'MB','Freq')))
ggplot(df.chosen, aes(x = val.rank, y = chosen.m, color = type.fac, group = type.fac)) +
geom_point(size = 3) +
geom_line(size = 1.5) +
geom_errorbar(aes(ymin = chosen.m - chosen.se, ymax = chosen.m + chosen.se), width = .2) +
xlab('') +
ylab('') +
scale_color_manual(values = c('#d11a02', '#105db0','#654321')) +
scale_y_continuous(breaks = c(0, .6), limits = c(0, .7)) +
scale_x_continuous(labels = NULL, breaks = c(1,5)) +
theme(legend.position = 'none')
# run multinomial logit
df2.logit = df2.filt %>% mutate(subject = as.numeric(subject))
for (i in 1:nrow(df2.logit)) {
s = df2.logit$subject[i]
if (i == 1 || s != last.s) {
fc = 1
}
df2.logit$food.id[i] = fc
fc = fc + 1
last.s = s
}
df2.logit2 = mlogit.data(df2.logit, choice = "chosen", shape = "long", id.var = "subject", alt.var = "food.id", chid.var = 'subject')
m.selection = mlogit(chosen ~ val.rank + val.spec.rank + freq.rank | -1, df2.logit2, panel = T,
rpar = c(val.rank = "n", val.spec.rank = "n", freq.rank = "n"), halton = NA, R = 1000, tol = .001)
m.selection.summary = summary(m.selection)
m.selection.summary
# For the mlogit model, there was no previous function to get the standardized coefficients, so I just did it myself.
# I'm not dividing by the sd of the dependent variable because it doesn't make sense for a multinomial logistic regression.
m.selection.summary$CoefTable[1,1] * sd(df2.logit2$val.rank)
m.selection.summary$CoefTable[1,2] * sd(df2.logit2$val.rank)
m.selection.summary$CoefTable[2,1] * sd(df2.logit2$val.spec.rank)
m.selection.summary$CoefTable[2,2] * sd(df2.logit2$val.spec.rank)
m.selection.summary$CoefTable[3,1] * sd(df2.logit2$freq.rank)
m.selection.summary$CoefTable[3,2] * sd(df2.logit2$freq.rank)
m.selection.null = mlogit(chosen ~ val.spec.rank + freq.rank | -1, df2.logit2, panel = T,
rpar = c(val.spec.rank = "n", freq.rank = "n"), halton = NA, R = 1000, tol = .001)
summary(m.selection.null)
ll1 = logLik(m.selection)
BIC1 = attr(ll1, 'df') * log(length(m.selection$fitted.values)) - 2 * as.numeric(ll1)
ll0 = logLik(m.selection.null)
BIC0 = attr(ll0, 'df') * log(length(m.selection.null$fitted.values)) - 2 * as.numeric(ll0)
BFnull.s1choice = exp((BIC1 - BIC0) / 2)
BFnull.s1choice
1 / BFnull.s1choice
m.selection.null2 = mlogit(chosen ~ val.rank + val.spec.rank | -1, df2.logit2, panel = T,
rpar = c(val.rank = "n", val.spec.rank = "n"), halton = NA, R = 1000, tol = .001)
ll0.freq = logLik(m.selection.null2)
BIC0.freq = attr(ll0.freq, 'df') * log(length(m.selection.null2$fitted.values)) - 2 * as.numeric(ll0.freq)
BFnull.freq = exp((BIC1 - BIC0.freq) / 2)
BFnull.freq
1 / BFnull.freq
save.image(expt.output)
rm(list=ls())
